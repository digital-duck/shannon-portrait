# Shannon Portrait â€” InfoCodec

**Information Coding & Encoding**: A research and educational tool for exploring Claude Shannon's Information Theory through practical image compression experiments.

## ðŸŽ¯ Project Overview

This project demonstrates Shannon's fundamental concepts through the thought experiment of transmitting a 2D image over a 1D wire:

- **2D â†’ 1D**: Encode/compress an image with one of six algorithms
- **Transmission**: Simulated 1D byte channel (the `.dat` file)
- **1D â†’ 2D**: Reconstruct the image and measure quality loss
- **Analysis**: Information theory metrics, visual diff, and AI-generated reports

## ðŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/shannon-portrait.git
cd shannon-portrait

# Create and activate environment
conda create -n st python=3.11
conda activate st

# Install in editable mode
pip install -e .
```

### CLI Usage

```bash
# Encode an image (auto-selects best method)
infocodec encode --input data/input/image/shannon-4.png

# Encode with a specific method
infocodec encode --input data/input/image/shannon-4.png --method huffman

# Decode back to an image
infocodec decode --input data/output/image/shannon-4_image_huffman.dat

# Benchmark all methods on an image
infocodec benchmark --input data/input/image/shannon-4.png --methods all

# Benchmark specific methods, output as Markdown
infocodec benchmark --input data/input/image/shannon-4.png --methods rle,huffman,dct --format markdown

# Launch the interactive web UI
infocodec-ui
```

### Streamlit UI

```bash
# Launch directly with Streamlit
streamlit run infocodec/ui/InfoCoDec.py

# Or via the installed entry point
infocodec-ui
```

The UI opens at **http://localhost:8501**.

## ðŸ“š Features

### Compression Methods

| Method | Type | Best For | Ratio | Quality |
|--------|------|----------|-------|---------|
| Naive | Baseline | Comparison only | 1.0Ã— | Perfect |
| RLE | Lossless | Solid regions, logos | 4â€“10Ã— | Perfect |
| Differential | Lossless | Smooth gradients | 2â€“5Ã— | Perfect |
| Huffman | Lossless | General purpose | 1.5â€“3Ã— | Perfect |
| Sparse | Lossy | Quick preview | 10â€“50Ã— | Variable |
| DCT | Lossy | Natural photos (JPEG-style) | 5â€“20Ã— | Adjustable |
| **auto** | â€” | Let the app decide | varies | varies |

**Auto-detection logic:**
- Unique pixel values < 32 â†’ RLE
- Entropy < 3.0 bits/pixel â†’ Differential
- Std deviation < 30 â†’ Differential
- Otherwise â†’ Huffman

### Information Theory Metrics

| Metric | Formula | Meaning |
|--------|---------|---------|
| Shannon Entropy | H(X) = -Î£ p(x) logâ‚‚(p(x)) | Theoretical minimum bits/symbol |
| PSNR | 10 logâ‚â‚€(255Â² / MSE) | Reconstruction fidelity (dB) |
| SSIM | structural + luminance + contrast | Perceptual similarity (0â€“1) |
| Coding Efficiency | H(X)Â·N / compressed_bits Ã— 100% | How close to Shannon limit |
| BPP | compressed_bits / num_pixels | Bits per pixel after compression |
| Compression Ratio | original_bytes / compressed_bytes | Size reduction factor |

### Multi-Page Streamlit UI

| Page | Description |
|------|-------------|
| âš™ï¸ Settings | Compression defaults, OpenRouter API key, LLM model selection |
| ðŸ—œï¸ Encode | Upload or generate an image, compress it, download `.dat` |
| ðŸ–¼ï¸ Decode | Reconstruct image from compressed data, view quality metrics |
| ðŸ“Š Diff | Visual diff, error heatmap, entropy analysis, rate-distortion |
| ðŸ“ Summarize | AI-generated report via OpenRouter (Claude, GPT-4, Gemini, etc.) |
| ðŸŽ“ Explain | Educational reference for all algorithms and metrics |

## ðŸ—ï¸ Project Structure

```
shannon-portrait/
â”œâ”€â”€ infocodec/                      # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                      # Click CLI (encode, decode, benchmark, ui)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ base.py                 # Abstract base classes + registry helpers
â”‚   â”‚   â”œâ”€â”€ metrics.py              # Shannon entropy, PSNR, SSIM, compression ratio
â”‚   â”‚   â”œâ”€â”€ compressors/
â”‚   â”‚   â”‚   â””â”€â”€ image/
â”‚   â”‚   â”‚       â”œâ”€â”€ naive.py        # Baseline â€” raw byte stream
â”‚   â”‚   â”‚       â”œâ”€â”€ rle.py          # Run-Length Encoding
â”‚   â”‚   â”‚       â”œâ”€â”€ differential.py # Delta / differential encoding
â”‚   â”‚   â”‚       â”œâ”€â”€ huffman.py      # Huffman variable-length coding
â”‚   â”‚   â”‚       â”œâ”€â”€ sparse.py       # Sparse sampling
â”‚   â”‚   â”‚       â””â”€â”€ dct.py          # Discrete Cosine Transform (JPEG-style)
â”‚   â”‚   â””â”€â”€ reconstructors/
â”‚   â”‚       â””â”€â”€ image/
â”‚   â”‚           â”œâ”€â”€ direct.py       # Naive reconstructor (reshape)
â”‚   â”‚           â”œâ”€â”€ rle.py          # RLE decoder
â”‚   â”‚           â”œâ”€â”€ differential.py # Cumulative-sum integrator
â”‚   â”‚           â”œâ”€â”€ huffman.py      # Huffman decoder
â”‚   â”‚           â”œâ”€â”€ sparse.py       # Interpolation-based reconstructor
â”‚   â”‚           â””â”€â”€ dct.py          # Inverse DCT reconstructor
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ InfoCoDec.py            # Streamlit app entry point
â”‚   â”‚   â”œâ”€â”€ components/             # Reusable UI components (future)
â”‚   â”‚   â””â”€â”€ pages/
â”‚   â”‚       â”œâ”€â”€ 1_âš™ï¸_Settings.py
â”‚   â”‚       â”œâ”€â”€ 2_ðŸ—œï¸_Encode.py
â”‚   â”‚       â”œâ”€â”€ 3_ðŸ–¼ï¸_Decode.py
â”‚   â”‚       â”œâ”€â”€ 4_ðŸ“Š_Diff.py
â”‚   â”‚       â”œâ”€â”€ 5_ðŸ“_Summarize.py
â”‚   â”‚       â””â”€â”€ 6_ðŸŽ“_Explain.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ image_utils.py          # load_image, save_image, create_test_image
â”‚       â”œâ”€â”€ paths.py                # Project-root resolution, output path builder
â”‚       â””â”€â”€ openrouter.py           # API key resolution + OpenAI-compat client
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â”œâ”€â”€ image/                  # Source images (Shannon portrait variants)
â”‚   â”‚   â”œâ”€â”€ audio/                  # Placeholder (Phase 2)
â”‚   â”‚   â””â”€â”€ text/                   # Placeholder (Phase 3)
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ image/                  # .dat compressed files + .json sidecars
â”‚       â”œâ”€â”€ audio/
â”‚       â””â”€â”€ text/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_compressors.py
â”‚   â”œâ”€â”€ test_metrics.py
â”‚   â””â”€â”€ test_reconstructors.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ CLAUDE.md
â”‚   â”œâ”€â”€ DIAGRAMS.md
â”‚   â”œâ”€â”€ VERIFICATION_GUIDE.md
â”‚   â””â”€â”€ FINAL_DELIVERY.md
â”œâ”€â”€ .env.example                    # API key template
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## ðŸ—‚ï¸ File Naming Convention

Compressed outputs follow a structured naming scheme so the method and modality are
readable at a glance without opening the file:

```
{stem}_{modality}_{method}[_{qualifier}].{ext}
```

| Component | Example | Notes |
|-----------|---------|-------|
| `stem` | `shannon-4` | Source filename without extension |
| `modality` | `image` | `image` / `audio` / `text` |
| `method` | `huffman` | The compression algorithm used |
| `qualifier` | `q08` / `sr4` | DCT quality (`q08` = 0.8) or Sparse rate (`sr4` = every 4th pixel) |
| `ext` | `dat` / `json` / `txt` | `.dat` = compressed binary; `.json` = metadata sidecar |

**Examples:**

```
shannon-4_image_huffman.dat        # Huffman-compressed image
shannon-4_image_dct_q08.dat        # DCT at quality 0.8
shannon-4_image_sparse_sr4.dat     # Sparse sampling, every 4th pixel
report_shannon-4_image_huffman.txt # Text report
summary_shannon-4_image_huffman.md # AI summary (Markdown)
metrics_shannon-4_image_huffman.json
```

## ðŸ“¦ Compressed File Format

Each `.dat` file is self-describing â€” no external configuration needed to decode it:

```
[4 bytes â€” metadata length (big-endian uint32)]
[N bytes â€” JSON metadata (method, original_shape, algorithm params)]
[remaining bytes â€” compressed payload]
```

A `.json` sidecar with the same stem is also written alongside each `.dat`
for easy inspection without decoding.

## âš™ï¸ CLI Reference

### `infocodec encode`

```
Usage: infocodec encode [OPTIONS]

Options:
  -i, --input PATH     Input image file [required]
  -m, --method TEXT    naive|rle|differential|huffman|sparse|dct|auto  [default: auto]
  -o, --output PATH    Output .dat file [default: data/output/image/...]
  -q, --quality FLOAT  Quality level 0.0â€“1.0 for DCT  [default: 1.0]
  -v, --verbose        Show detailed compression stats
```

### `infocodec decode`

```
Usage: infocodec decode [OPTIONS]

Options:
  -i, --input PATH   Compressed .dat file [required]
  -o, --output PATH  Output image file  [default: data/output/image/..._reconstructed.png]
  -v, --verbose      Show reconstruction stats
```

### `infocodec benchmark`

```
Usage: infocodec benchmark [OPTIONS]

Options:
  -i, --input PATH    Input image file [required]
  -m, --methods TEXT  Comma-separated methods or "all"  [default: all]
  -o, --output PATH   Output directory for results
  -f, --format TEXT   table|json|markdown  [default: table]
```

### `infocodec-ui`

Launches the Streamlit web interface at `http://localhost:8501`.

## ðŸ¤– LLM Integration (OpenRouter)

The **ðŸ“ Summarize** page generates AI-powered experiment reports.

### Setup

```bash
# Option 1 â€” .env file (recommended)
cp .env.example .env
# Edit .env and set:  OPENROUTER_API_KEY=sk-or-...

# Option 2 â€” shell export
export OPENROUTER_API_KEY=sk-or-...
```

Resolution order: `.env` file â†’ `OPENROUTER_API_KEY` env var â†’ manual entry in Settings.

### Supported Models

- `anthropic/claude-3.5-sonnet` (default)
- `anthropic/claude-3-opus`
- `anthropic/claude-3-haiku`
- `openai/gpt-4-turbo`
- `openai/gpt-4`
- `google/gemini-pro`
- `meta-llama/llama-3-70b-instruct`
- Any model available on [openrouter.ai](https://openrouter.ai/models)

### Report Styles

| Style | Audience |
|-------|---------|
| Technical | Research papers, engineering reports |
| Educational | Students, classroom demos |
| Executive Summary | Quick decision-makers |

The prompt is fully editable before generation. Reports can be downloaded as `.md` or `.txt`.

## ðŸ“– Theoretical Background

### Shannon's Three Theorems

**Entropy** â€” the information content of a source:
```
H(X) = -Î£ p(x) logâ‚‚(p(x))     bits/symbol
```
This is the *theoretical lower bound* for lossless compression. No algorithm can compress below it.

**Channel Capacity** â€” maximum reliable transmission rate:
```
C = B logâ‚‚(1 + S/N)            bits/second
```

**Rateâ€“Distortion** â€” the fundamental lossy trade-off:
```
R(D) = min I(X;Y)  subject to  E[d(X,Y)] â‰¤ D
```
More compression (lower rate) always means more distortion. This app makes this trade-off
visible through the DCT quality slider and the Diff page.

## ðŸ”¬ Python API

```python
from infocodec.core.compressors import COMPRESSORS
from infocodec.core.reconstructors import RECONSTRUCTORS
from infocodec.core.metrics import calculate_entropy, comprehensive_quality_analysis
from infocodec.utils.image_utils import load_image, save_image

# Load image (returns uint8 ndarray, grayscale or RGB)
image = load_image("data/input/image/shannon-4.png")

# Compress
compressor = COMPRESSORS['huffman']()
compressed_bytes, metadata = compressor.compress(image)
print(compressor.get_stats())

# Reconstruct
reconstructor = RECONSTRUCTORS['huffman']()
reconstructed = reconstructor.reconstruct(compressed_bytes, metadata)

# Analyse
analysis = comprehensive_quality_analysis(
    original=image,
    reconstructed=reconstructed,
    original_size=image.size,
    compressed_size=len(compressed_bytes),
)
print(f"PSNR: {analysis['psnr_db']:.2f} dB")
print(f"SSIM: {analysis['ssim']:.4f}")
print(f"Efficiency: {analysis['efficiency_percent']:.1f}%")
```

### Benchmark all methods in code

```python
for name, cls in COMPRESSORS.items():
    c = cls()
    cb, meta = c.compress(image)
    stats = c.get_stats()
    print(f"{name:15} ratio={stats.get('compression_ratio', 0):.2f}x  entropy={stats.get('entropy', 0):.2f}")
```

## ðŸ§ª Running Tests

```bash
# Run all tests
pytest

# With coverage report
pytest --cov=infocodec --cov-report=html

# Run a specific suite
pytest tests/test_compressors.py -v
pytest tests/test_metrics.py -v
pytest tests/test_reconstructors.py -v
```

## ðŸ“Š Example Results

### Shannon Portrait (shannon-4.png â€” colour, 240Ã—179)

| Method | Ratio | Entropy (bpp) | PSNR | Type |
|--------|-------|---------------|------|------|
| Naive | 1.00Ã— | 7.54 | âˆž | Lossless |
| RLE | ~0.5Ã— | 7.54 | âˆž | Lossless |
| Differential | ~2.0Ã— | ~1.2 | âˆž | Lossless |
| Huffman | ~1.2Ã— | 7.54 | âˆž | Lossless |
| Sparse (sr=4) | ~16Ã— | â€” | ~28 dB | Lossy |
| DCT (q=0.8) | ~8Ã— | â€” | ~35 dB | Lossy |

### Synthetic Gradient (64Ã—64, grayscale)

| Method | Ratio | Entropy | PSNR |
|--------|-------|---------|------|
| Naive | 1.00Ã— | 6.72 | âˆž |
| RLE | 0.50Ã— | 6.72 | âˆž |
| Differential | 8.93Ã— | 0.28 | âˆž |
| Huffman | 1.19Ã— | 6.72 | âˆž |

### Synthetic Blocks (64Ã—64, grayscale)

| Method | Ratio | Entropy | PSNR |
|--------|-------|---------|------|
| Naive | 1.00Ã— | 5.72 | âˆž |
| RLE | 4.00Ã— | 5.72 | âˆž |
| Differential | 2.19Ã— | 1.29 | âˆž |
| Huffman | 1.40Ã— | 5.72 | âˆž |

## ðŸ”® Future Enhancements

### Phase 2 â€” Audio Support

- PCM, ADPCM, simplified LPC compressors
- Waveform + spectrogram visualisation
- Perceptual audio metrics (SNR, spectral distortion)

### Phase 3 â€” Text Support

- LZ77 / LZ78 / LZW algorithms
- Huffman and arithmetic coding for text
- Language-model-based compression (tokenisation entropy)

### Phase 4 â€” Advanced Features

- Real-time streaming simulation with latency/packet-loss injection
- Error correction codes (Hamming, Reed-Solomon)
- Comparative studies database
- Arithmetic coding (approaches entropy within 0.001 bits)

## ðŸ¤ Contributing

Contributions welcome! Areas of interest:

- New compression algorithms (arithmetic coding, LZ77, LZMA)
- Audio / text modality support (architecture already in place)
- UI improvements and new visualisations
- Performance optimisation (numba, Cython for inner loops)
- Additional test coverage

## ðŸ“„ License

MIT License â€” see [LICENSE](LICENSE).

## ðŸ™ Acknowledgments

- **Claude Shannon** â€” for founding information theory
- **Cover & Thomas** â€” *Elements of Information Theory* (textbook reference)
- **David MacKay** â€” *Information Theory, Inference, and Learning Algorithms* (freely available online)

## ðŸ˜ Broader Context: Elegant Elephant

This project is one instrument in a larger research programme called **Elegant Elephant**,
which proposes that compression, dimensionality reduction, quantum measurement, symmetry
breaking, and philosophical notions of illusion are all the *same underlying process* â€”
projection from a higher-dimensional reality to a lower-dimensional observable.

Within that framework, `shannon-portrait` studies the **information theory axis**:

| Elegant Elephant concept | This project's concrete form |
|---|---|
| Projection operator `Ï€: H â†’ L` | Each compression algorithm (DCT, Huffman, RLE, â€¦) |
| Information loss `I(H) > I(Ï€(H))` | Entropy, PSNR, SSIM, compression ratio metrics |
| Irreversibility of projection | Lossy vs lossless reconstruction experiments |
| Different projections reveal different aspects | Benchmark comparison across all methods |

The manifold learning axis (PHATE, t-SNE, PCA as projection operators) is studied in a
sibling instrument: **Semanscope**.

See [`README-Elegant-Elephant.md`](../zinets/README-Elegant-Elephant.md) for the full unified framework.

---

*"The fundamental problem of communication is that of reproducing at one point exactly or approximately a message selected at another."* â€” Claude Shannon, 1948
