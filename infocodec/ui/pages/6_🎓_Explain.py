"""
Explain Page
============

Educational reference for all algorithms and concepts used in InfoCodec.
"""

import streamlit as st

st.set_page_config(page_title="Explain - InfoCodec", page_icon="ğŸ“", layout="wide")

st.title("ğŸ“ Explain")
st.markdown("A guided tour of Shannon's Information Theory and every algorithm used in this app.")

# â”€â”€ Navigation helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab1, tab2, tab3, tab4 = st.tabs([
    "ğŸ“¡ Information Theory",
    "ğŸ”’ Lossless Algorithms",
    "âœ‚ï¸ Lossy Algorithms",
    "ğŸ“ Quality Metrics",
])

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 1 â€” Information Theory Foundations
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab1:
    st.header("ğŸ“¡ Shannon's Information Theory")

    st.markdown("""
    Claude Shannon's 1948 paper *"A Mathematical Theory of Communication"* created the
    entire field of information theory. Three ideas underpin everything in this app.
    """)

    # â”€â”€ Entropy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ² Shannon Entropy â€” H(X)", expanded=True):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it is:** The average amount of *surprise* (information) per symbol in a source.

            **Formula:**
            ```
            H(X) = -Î£ p(x) Â· logâ‚‚(p(x))    (bits per symbol)
            ```

            **Intuition:**
            - A fair coin flip: H = 1 bit (maximum uncertainty)
            - A biased coin (99% heads): H â‰ˆ 0.08 bits (very predictable)
            - A uniform 256-value image: H = 8 bits/pixel (maximally random)
            - A smooth gradient image: H â‰ˆ 3â€“5 bits/pixel (very compressible)

            **Why it matters for compression:**
            Entropy is the *theoretical lower bound* on how many bits per symbol any
            lossless compressor can achieve. You cannot compress below entropy without
            losing information.

            **In this app:** Shown on the Encode page and used to calculate Coding Efficiency
            on the Diff page.
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Entropy (information theory)](https://en.wikipedia.org/wiki/Entropy_(information_theory))")
            st.markdown("[YouTube â€” Search: Shannon entropy explained](https://www.youtube.com/results?search_query=shannon+entropy+information+theory+explained)")
            st.markdown("[YouTube â€” 3Blue1Brown: Wordle & information](https://www.youtube.com/results?search_query=3blue1brown+information+theory+wordle)")

    # â”€â”€ Source Coding Theorem â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“œ Shannon's Source Coding Theorem"):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it says:** It is possible to compress a source to an average code length
            arbitrarily close to its entropy H(X), but no further (losslessly).

            **Formally:**
            ```
            H(X) â‰¤ L < H(X) + 1      where L = average code length
            ```

            **Practical meaning:**
            - Huffman coding approaches the entropy limit within 1 bit per symbol
            - Arithmetic coding can get within 0.001 bits
            - No lossless code can beat H(X)

            **Coding Efficiency** (shown on the Diff page):
            ```
            Efficiency = (H(X) Ã— N) / (compressed bits) Ã— 100%
            ```
            where N is the number of pixels. Values above 95% are excellent.
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem)")
            st.markdown("[YouTube â€” Search: source coding theorem](https://www.youtube.com/results?search_query=shannon+source+coding+theorem+compression)")

    # â”€â”€ Channel Capacity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“¶ Channel Capacity â€” C"):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it is:** The maximum rate at which information can be reliably transmitted
            over a noisy channel.

            **Shannon-Hartley formula:**
            ```
            C = B Â· logâ‚‚(1 + S/N)    (bits per second)
            ```
            where B = bandwidth (Hz), S/N = signal-to-noise ratio.

            **Intuition:** A wider pipe (B) or less noise (high S/N) â†’ more capacity.
            This is why fibre optic cables with low noise carry so much data.

            **Connection to PSNR:** PSNR is essentially a measurement of S/N after
            compression. High PSNR = low distortion = the reconstructed image is a
            faithful copy of what was "transmitted" through the compression channel.
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Channel capacity](https://en.wikipedia.org/wiki/Channel_capacity)")
            st.markdown("[Wikipedia â€” Shannonâ€“Hartley theorem](https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem)")
            st.markdown("[YouTube â€” Search: channel capacity Shannon](https://www.youtube.com/results?search_query=channel+capacity+shannon+hartley+explained)")

    # â”€â”€ Rateâ€“Distortion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“‰ Rateâ€“Distortion Theory"):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it is:** The fundamental trade-off between how many bits you use (rate)
            and how much quality you lose (distortion).

            **Key insight:**
            - For *lossless* compression, rate â‰¥ H(X) â€” you can never go below entropy
            - For *lossy* compression, you can go below H(X) by accepting distortion
            - The rate-distortion function R(D) gives the minimum rate needed to achieve
              distortion D

            **In practice:**
            | Method | Rate | Distortion |
            |--------|------|------------|
            | Naive  | 8 bpp | 0 |
            | Huffman | ~H(X) bpp | 0 |
            | DCT (quality=1.0) | ~2â€“4 bpp | low |
            | DCT (quality=0.1) | ~0.5 bpp | high |
            | Sparse (rate=4) | ~0.5 bpp | medium |

            **Shown on the Diff page** under "Rateâ€“Distortion Trade-off".
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Rateâ€“distortion theory](https://en.wikipedia.org/wiki/Rate%E2%80%93distortion_theory)")
            st.markdown("[YouTube â€” Search: rate distortion theory explained](https://www.youtube.com/results?search_query=rate+distortion+theory+compression)")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 2 â€” Lossless Algorithms
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab2:
    st.header("ğŸ”’ Lossless Compression Algorithms")
    st.markdown("""
    Lossless algorithms reconstruct the image *exactly*. After decode, PSNR = âˆ and SSIM = 1.0000.
    The trade-off is that you are bounded by Shannon entropy â€” you cannot compress below H(X).
    """)

    # â”€â”€ Naive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("â¬œ Naive â€” Baseline (1.0Ã—)", expanded=True):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it does:** No compression at all â€” just flattens the 2D pixel array
            into a 1D byte stream. Used as a *baseline* to compare all other methods.

            **How it works:**
            ```
            image (HÃ—W) â†’ flatten â†’ bytes [p0, p1, p2, ...]
            ```

            **Steps:**
            1. Cast pixels to `uint8`
            2. Call `array.tobytes()` â€” the "compressed" stream IS the raw pixels
            3. Store shape in metadata for reconstruction

            **When to use:** Never for real compression. Useful as a reference point â€”
            any other method should do better on compressible images.

            **Compression ratio:** Always 1.0Ã— (no change in size)
            **Quality:** Perfect (PSNR = âˆ)
            **Type:** Lossless
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Lossless compression](https://en.wikipedia.org/wiki/Lossless_compression)")

    # â”€â”€ RLE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("â–¬â–¬â–¬ RLE â€” Run-Length Encoding (4â€“10Ã—)"):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it does:** Replaces consecutive identical values with a (value, count) pair.

            **Example:**
            ```
            Raw pixels:  [255, 255, 255, 255, 0, 0, 128, 128, 128]
            RLE encoded: [(255,4), (0,2), (128,3)]
            Bytes saved: 9 â†’ 6  (2 bytes per pair vs 1 byte per pixel for runs â‰¥ 2)
            ```

            **How it works in this app:**
            ```python
            current_val, count = flat_data[0], 1
            for val in flat_data[1:]:
                if val == current_val and count < 255:
                    count += 1
                else:
                    emit(current_val, count)
                    current_val, count = val, 1
            emit(current_val, count)   # flush last run
            ```

            **When it wins:** Images with large uniform regions â€” logos, diagrams,
            pixel art, screenshots with solid backgrounds.

            **When it loses:** Noisy or photographic images â€” every pixel differs,
            so every run has length 1 â†’ output is *2Ã— larger* than input.

            **Compression ratio:** 4â€“10Ã— on block patterns; < 1Ã— on noise
            **Quality:** Perfect (lossless)
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Run-length encoding](https://en.wikipedia.org/wiki/Run-length_encoding)")
            st.markdown("[YouTube â€” Search: run length encoding](https://www.youtube.com/results?search_query=run+length+encoding+compression+explained)")

    # â”€â”€ Differential â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“ˆ Differential â€” Delta Encoding (2â€“5Ã—)"):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it does:** Instead of storing absolute pixel values, stores the
            *difference* between consecutive pixels (delta encoding).

            **Example:**
            ```
            Raw pixels:   [100, 102, 105, 104, 103]
            Differences:  [100,  +2,  +3,  -1,  -1]   â† much smaller values!
            Entropy of differences << Entropy of raw pixels
            ```

            **How it works:**
            ```
            Encode:  diff[0] = pixel[0]
                     diff[i] = pixel[i] - pixel[i-1]   (stored as int16)

            Decode:  pixel = cumsum(diff)               (cumulative sum)
            ```

            **Why it reduces entropy:** In a smooth image, neighbouring pixels are
            similar â€” differences cluster tightly around 0, making the distribution
            much more predictable (lower entropy) than the raw values.

            **Entropy reduction** (shown in Diff page) can be 70â€“95% on gradients.

            **When it wins:** Smooth gradients, natural photographs, time-series data.
            **When it loses:** High-frequency noise â€” differences are as large as the values.

            **Compression ratio:** 2â€“5Ã— on smooth images
            **Quality:** Perfect (lossless)
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Delta encoding](https://en.wikipedia.org/wiki/Delta_encoding)")
            st.markdown("[Wikipedia â€” Predictive coding](https://en.wikipedia.org/wiki/Predictive_coding)")
            st.markdown("[YouTube â€” Search: delta encoding differential compression](https://www.youtube.com/results?search_query=delta+encoding+differential+compression+tutorial)")

    # â”€â”€ Huffman â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸŒ² Huffman Coding â€” Variable-Length Codes (1.5â€“3Ã—)"):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it does:** Assigns shorter bit codes to frequent symbols and longer
            codes to rare symbols â€” optimally exploiting the probability distribution.

            **Example (frequency-ordered):**
            ```
            Symbol | Frequency | Fixed (8 bits) | Huffman code
            -------|-----------|----------------|-------------
               200 |     40%   |   11001000     |     0       (1 bit)
               128 |     30%   |   10000000     |    10       (2 bits)
                50 |     20%   |   00110010     |   110       (3 bits)
                 0 |     10%   |   00000000     |   111       (3 bits)
            Average: 0.4Ã—1 + 0.3Ã—2 + 0.2Ã—3 + 0.1Ã—3 = 1.9 bits  vs  8 bits fixed
            ```

            **How the tree is built (greedy algorithm):**
            ```
            1. Create one leaf node per unique symbol, weighted by frequency
            2. Repeat until one tree remains:
               a. Pop the two lowest-frequency nodes
               b. Merge into a parent node (sum of frequencies)
               c. Push parent back into the priority queue
            3. Assign 0 to left branches, 1 to right branches
            4. Each leaf's path from root = its codeword
            ```

            **Shannon's guarantee:** Huffman achieves H(X) â‰¤ L < H(X) + 1 bits/symbol.
            In practice this app reaches **97â€“99% coding efficiency**.

            **In this app:** The code table is serialised into the `.dat` file header
            alongside the compressed bitstream, enabling self-contained decoding.

            **Compression ratio:** 1.5â€“3Ã— for typical images
            **Quality:** Perfect (lossless)
            **Efficiency:** 97â€“99% of Shannon limit
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding)")
            st.markdown("[YouTube â€” Search: Huffman coding tree algorithm](https://www.youtube.com/results?search_query=huffman+coding+tree+algorithm+explained)")
            st.markdown("[YouTube â€” Computerphile: Huffman](https://www.youtube.com/results?search_query=computerphile+huffman+encoding)")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 3 â€” Lossy Algorithms
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab3:
    st.header("âœ‚ï¸ Lossy Compression Algorithms")
    st.markdown("""
    Lossy algorithms trade quality for much higher compression ratios. They deliberately
    discard information that is hard to perceive â€” the decoded image is an *approximation*
    of the original. PSNR and SSIM measure how much quality is lost.
    """)

    # â”€â”€ Sparse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ¯ Sparse Sampling â€” Subsample & Interpolate (10â€“50Ã—)", expanded=True):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it does:** Keeps only every N-th pixel (spatial subsampling) and
            reconstructs the missing pixels by interpolation.

            **How it works:**
            ```
            Encode:
              for row in range(0, H, N):
                for col in range(0, W, N):
                  store (row, col, pixel_value)   â† only NÂ² pixels per NÃ—N block

            Decode:
              place sampled pixels at known positions
              fill gaps using scipy.interpolate.griddata  (nearest / linear / cubic)
            ```

            **Visual intuition:**
            ```
            Original 4Ã—4 (N=2):        Sampled (every 2nd):     Interpolated:
            A B C D                     A . C .                  A A C C
            E F G H         â†’           . . . .         â†’        A A C C
            I J K L                     I . K .                  I I K K
            M N O P                     . . . .                  I I K K
            ```

            **Why it loses quality:** Interpolation guesses missing values â€” it works
            well for smooth regions but blurs sharp edges and fine texture.

            **Compression ratio:** NÂ² (e.g. N=4 â†’ 16Ã—, N=8 â†’ 64Ã—)
            **Quality:** Depends on N and image smoothness
            **Type:** Lossy
            **Note:** Currently operates on grayscale (2D coordinate system)
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Downsampling (signal processing)](https://en.wikipedia.org/wiki/Downsampling_(signal_processing))")
            st.markdown("[Wikipedia â€” Image scaling / interpolation](https://en.wikipedia.org/wiki/Image_scaling)")
            st.markdown("[YouTube â€” Search: image subsampling interpolation](https://www.youtube.com/results?search_query=image+subsampling+interpolation+compression)")

    # â”€â”€ DCT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸŒŠ DCT â€” Discrete Cosine Transform / JPEG-style (5â€“20Ã—)"):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it does:** Converts image blocks from the *spatial domain* (pixel values)
            to the *frequency domain* (cosine wave amplitudes), then discards high-frequency
            components that the human eye barely notices.

            **This is the core idea behind JPEG.**

            **Step-by-step pipeline:**
            ```
            1. SPLIT  â€” divide image into 8Ã—8 pixel blocks
            2. DCT    â€” apply 2D Discrete Cosine Transform to each block
                        [spatial pixels] â†’ [64 frequency coefficients]
                        coefficient (0,0) = DC (average brightness)
                        coefficient (i,j) = amplitude of cosine wave at frequency (i,j)
            3. QUANTISE â€” divide each coefficient by a quality-dependent step size
                        Low frequencies (top-left) â†’ fine steps  (preserve)
                        High frequencies (bottom-right) â†’ coarse steps  (discard)
                        Many high-freq coefficients â†’ 0  (run of zeros â†’ compressible)
            4. ENCODE â€” store non-zero coefficients (float32 in this app; zigzag+
                        entropy coding in full JPEG)
            5. DECODE â€” reverse: dequantise â†’ IDCT â†’ reassemble blocks
            ```

            **DCT formula for a 1D block of N values:**
            ```
            X[k] = Î£ x[n] Â· cos(Ï€/N Â· (n + 0.5) Â· k)    k = 0â€¦N-1
            ```

            **Why DCT beats Fourier for images:** DCT uses only real cosines (no complex
            numbers) and avoids the "ringing" artefacts of DFT at block boundaries.

            **Quality parameter (this app):**
            - `quality=1.0` â†’ fine quantisation â†’ low loss, moderate compression
            - `quality=0.1` â†’ coarse quantisation â†’ blocky artefacts, high compression

            **Compression ratio:** 5â€“20Ã— depending on quality setting
            **Quality:** Adjustable (PSNR 20â€“45 dB)
            **Type:** Lossy
            **Note:** Currently operates on grayscale; real JPEG applies DCT per YCbCr channel
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Discrete cosine transform](https://en.wikipedia.org/wiki/Discrete_cosine_transform)")
            st.markdown("[Wikipedia â€” JPEG compression](https://en.wikipedia.org/wiki/JPEG)")
            st.markdown("[Wikipedia â€” Quantization (signal processing)](https://en.wikipedia.org/wiki/Quantization_(signal_processing))")
            st.markdown("[YouTube â€” Search: how JPEG compression works DCT](https://www.youtube.com/results?search_query=how+jpeg+compression+works+dct+explained)")
            st.markdown("[YouTube â€” Computerphile: JPEG](https://www.youtube.com/results?search_query=computerphile+how+jpeg+works)")

    # â”€â”€ Comparison table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("---")
    st.subheader("ğŸ“Š Algorithm Comparison")
    st.markdown("""
    | Algorithm | Type | Ratio | Quality | Best For | Worst On |
    |-----------|------|-------|---------|----------|----------|
    | Naive | Lossless | 1Ã— | Perfect | Baseline only | â€” |
    | RLE | Lossless | 4â€“10Ã— | Perfect | Solid regions, logos | Noise, photos |
    | Differential | Lossless | 2â€“5Ã— | Perfect | Smooth gradients | High-freq noise |
    | Huffman | Lossless | 1.5â€“3Ã— | Perfect | General purpose | Already-compressed data |
    | Sparse | Lossy | 10â€“50Ã— | Variable | Quick preview | Sharp edges, text |
    | DCT | Lossy | 5â€“20Ã— | Adjustable | Natural photos | Hard edges, text |
    """)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TAB 4 â€” Quality Metrics
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with tab4:
    st.header("ğŸ“ Quality & Compression Metrics")
    st.markdown("These metrics are computed on the **Diff** page after encode + decode.")

    # â”€â”€ PSNR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“¡ PSNR â€” Peak Signal-to-Noise Ratio", expanded=True):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it measures:** How much noise (error) was introduced by compression,
            expressed as a ratio between the maximum possible signal and the error power.

            **Formula:**
            ```
            MSE  = mean( (original - reconstructed)Â² )
            PSNR = 10 Â· logâ‚â‚€(255Â² / MSE)              (dB)
            ```
            If MSE = 0 (perfect match), PSNR = âˆ.

            **Interpretation guide:**
            | PSNR | Quality |
            |------|---------|
            | âˆ dB | Perfect â€” lossless reconstruction |
            | > 40 dB | Excellent â€” imperceptible difference |
            | 30â€“40 dB | Good â€” minor artefacts visible on close inspection |
            | 20â€“30 dB | Acceptable â€” noticeable artefacts |
            | < 20 dB | Poor â€” significant quality loss |

            **Limitation:** PSNR is a mathematical measure â€” it does not always match
            human perception. A blurry image and a sharp but shifted image might have the
            same PSNR but look very different to a person. SSIM addresses this.
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)")
            st.markdown("[YouTube â€” Search: PSNR image quality metric](https://www.youtube.com/results?search_query=PSNR+peak+signal+noise+ratio+image+quality)")

    # â”€â”€ SSIM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ‘ï¸ SSIM â€” Structural Similarity Index"):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **What it measures:** Perceptual similarity â€” how similar two images *look*
            to a human viewer, accounting for luminance, contrast, and structure.

            **Formula (per local window):**
            ```
            SSIM(x,y) = [luminance] Â· [contrast] Â· [structure]
                      = (2Î¼â‚“Î¼áµ§ + Câ‚)(2Ïƒâ‚“áµ§ + Câ‚‚)
                        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        (Î¼â‚“Â² + Î¼áµ§Â² + Câ‚)(Ïƒâ‚“Â² + Ïƒáµ§Â² + Câ‚‚)
            ```
            where Î¼ = local mean, Ïƒ = local variance, Câ‚/Câ‚‚ = stability constants.

            **Interpretation:**
            | SSIM | Meaning |
            |------|---------|
            | 1.0000 | Identical images |
            | > 0.95 | Very similar â€” differences barely visible |
            | > 0.90 | Similar â€” subtle differences |
            | 0.70â€“0.90 | Noticeable differences |
            | < 0.70 | Significant structural loss |

            **Advantage over PSNR:** SSIM is correlated with human visual perception.
            Blurring reduces SSIM more than a uniform brightness shift of the same MSE,
            matching how humans actually perceive image quality degradation.
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” SSIM](https://en.wikipedia.org/wiki/Structural_similarity_index_measure)")
            st.markdown("[YouTube â€” Search: SSIM structural similarity image quality](https://www.youtube.com/results?search_query=SSIM+structural+similarity+index+image+quality)")

    # â”€â”€ Entropy & Efficiency â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("âš¡ Coding Efficiency â€” How Close to the Theoretical Limit?"):
        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown("""
            **Theoretical minimum bits:**
            ```
            Theoretical minimum = H(X) Ã— N_pixels     (bits)
            ```

            **Coding efficiency:**
            ```
            Efficiency = Theoretical minimum / Actual compressed bits Ã— 100%
            ```

            **Example:** An image with H(X) = 5.0 bits/pixel and 10,000 pixels has a
            theoretical minimum of 50,000 bits. If Huffman encodes it in 51,546 bits:
            ```
            Efficiency = 50,000 / 51,546 Ã— 100% = 97.0%
            ```

            **Interpretation:**
            | Efficiency | Meaning |
            |------------|---------|
            | > 95% | Excellent â€” near-optimal |
            | 80â€“95% | Good |
            | 60â€“80% | Moderate â€” room for improvement |
            | < 60% | Low â€” significant overhead (headers, metadata) |

            **Why it's never 100%:** Real compressors have metadata overhead (code tables,
            shape info). Arithmetic coding can get very close; Huffman loses at most
            1 bit/symbol.
            """)
        with col2:
            st.markdown("**Learn more:**")
            st.markdown("[Wikipedia â€” Data compression ratio](https://en.wikipedia.org/wiki/Data_compression_ratio)")
            st.markdown("[Wikipedia â€” Kraft's inequality](https://en.wikipedia.org/wiki/Kraft%27s_inequality)")

    # â”€â”€ BPP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ”¢ BPP â€” Bits Per Pixel"):
        st.markdown("""
        **What it measures:** How many bits of storage each pixel requires after compression.

        ```
        BPP = (compressed size in bytes Ã— 8) / number of pixels
        ```

        **Reference values:**
        | BPP | Meaning |
        |-----|---------|
        | 8.0 | Uncompressed grayscale (1 byte per pixel) |
        | 24.0 | Uncompressed RGB (3 bytes per pixel) |
        | 0.5â€“2.0 | Good JPEG compression |
        | < 0.5 | Aggressive lossy compression |
        | ~H(X) | Ideal lossless (e.g. Huffman on a 5-bit-entropy image â†’ 5 bpp) |

        **In the Diff page:** Shown as both *Original BPP* (always 8) and
        *Compressed BPP* (the achieved rate).

        [Wikipedia â€” Bits per pixel](https://en.wikipedia.org/wiki/Bit_depth)
        """)

    # â”€â”€ MSE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“ MSE â€” Mean Squared Error"):
        st.markdown("""
        **What it measures:** The average squared difference between original and
        reconstructed pixel values. The foundation of PSNR.

        ```
        MSE = (1/N) Â· Î£ (original_i - reconstructed_i)Â²
        ```

        **Properties:**
        - MSE = 0 â†’ perfect reconstruction
        - MSE is sensitive to large errors (squaring penalises outliers heavily)
        - Lower is always better
        - Units: (pixel value)Â² â€” not directly interpretable; PSNR converts it to dB

        **Limitation:** Like PSNR, MSE doesn't match human perception well.
        Two images with the same MSE can look very different. Use together with SSIM
        for a complete picture.

        [Wikipedia â€” Mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error)
        """)

    # â”€â”€ Compression Ratio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ“¦ Compression Ratio & Space Saved"):
        st.markdown("""
        **Compression ratio:**
        ```
        Ratio = original size / compressed size
        ```
        A ratio of 4Ã— means the compressed file is 4 times smaller than the original.

        **Space saved:**
        ```
        Space saved = (1 - 1/ratio) Ã— 100%
        ```
        | Ratio | Space saved |
        |-------|-------------|
        | 1Ã— | 0% |
        | 2Ã— | 50% |
        | 4Ã— | 75% |
        | 8Ã— | 87.5% |
        | 16Ã— | 93.75% |

        **Note:** Compression ratio can be < 1Ã— (expansion) for noisy data compressed
        with RLE â€” the run-length pairs take more space than the raw bytes.

        [Wikipedia â€” Data compression ratio](https://en.wikipedia.org/wiki/Data_compression_ratio)
        """)

# â”€â”€ Footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #888;'>
    <em>"The fundamental problem of communication is that of reproducing at one point
    exactly or approximately a message selected at another." â€” Claude Shannon, 1948</em>
</div>
""", unsafe_allow_html=True)
